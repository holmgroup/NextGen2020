{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Up until this point, we've explored raw image representations, histogram representations, transfer learning, and unsupervised learning. Today, we'll dive into the world of supervised learning.\n",
    "\n",
    "Supervised learning is the process of training a machine learning system with data (X) that has corresponding targets (y). Recall that linear regression on an X-Y scatter plot is the simplest example of supervised learning. In that case, you have some data X that you try to fit a linear model (or polynomial, exponential, etc.) such that you minimize the sum of squared error (distance) between `Y` and `f(x)`.\n",
    "\n",
    "Here, we'll revisit our familiar NEU Surface Defect Dataset and try supervised learning with a support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import some things\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from helper import visualize as vis\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = os.path.join('..', 'data', 'df_histeq_and_cnn.pickle')\n",
    "df = pd.read_pickle(DF_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in our dataframe?\n",
    "Over the last week, we've built this dataframe that connects the image paths to the images, their histograms, histogram-equalized images, the equalized histograms, and the outputs from the first fully connected layer of VGG16. All of these (minus the file paths) are suitable entries for our `X` data and the column `label_x` will be our `y` in classifier system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's shuffle our data\n",
    "df = shuffle(df, random_state=21213)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I'm encoding the labels into numeric form.\n",
    "le = LabelEncoder()\n",
    "le.fit(df['label_x'])\n",
    "label_y = le.transform(df['label_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['label_x'].unique())\n",
    "print(le.transform(df['label_x'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the ordering... we'll come back this when we visualize\n",
    "print(le.transform(['Cr', 'In', 'PS', 'Pa', 'RS', 'Sc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's split our data into a training set and a testing set\n",
    "# it's worth noting that train_test_split has a default parameter to shuffle the data\n",
    "# so the above code block could be eliminated\n",
    "# I just wanted to make a point\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['vgg16_fc1_feature'], \n",
    "                                                    label_y,\n",
    "                                                    test_size=0.3333,\n",
    "                                                    random_state=14,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the structure of the data\n",
    "X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.to_numpy()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma='auto', random_state=4)\n",
    "clf.fit(X_train.to_numpy(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! It looks like there's a warning message about our data structure. That's okay. We kind of thought something was off. We can fix that with a couple of one-liners. `X_train` and `X_test` are both arrays of arrays, but there's an easy fix to transform them each into a 2-D array. First we'll convert the array to a list, then that list will be re-transformed to a NumPy array, which will automatically shape the array into the form that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(list(X_train))\n",
    "X_test = np.array(list(X_test))\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = SVC(kernel='rbf', gamma='auto', random_state=4)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our classifier obtains nearly perfect performance! Yay! However, as you venture further into your career, you'll become increasingly skeptical of results like this--especially when they come so easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_test, y_pred=clf.predict(X_test))\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5), dpi=150)\n",
    "vis.pretty_cm(cm, ax0=ax, labelnames=['Cr', 'In', 'PS', 'Pa', 'RS', 'Sc'])\n",
    "ax.set_title('SVM Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "We can increase our confidence in the support vector machine by applying a technique called K-Fold cross validation. To this point, we've only trained a classifier once and found that it had pretty good performance. But what if we got lucky? How do we eliminate sheer dumb luck as an explanation?\n",
    "\n",
    "Cross validation is the process of trying various train/test splits so we can study the variance in classifier performance across various training sets. In this case, I propose we leave our `X_test` and `y_test` data alone. Instead, we'll perform various splits on the remaining `X_train` and `y_train` data.\n",
    "\n",
    "The K in K-Fold Cross Validation refers to the number of times that we'll split the data. In the case of 5-Fold cross validation, we'll split the data into 5 equal chunks. We can save the first chunk as testing data, and train on the remaining four chunks. Then we'll start over, but this time hold out the second chunk for testing while training on the remaining four. We'll repeat this for a total of five times such that each chunk is selected once to be the testing data after being trained on the other data. As I mentioned before, I want to keep the actual testing data completely hidden, so from here on I'll refer to this interim testing data as *validation* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "kf_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val  = y[train_index], y[val_index]\n",
    "    \n",
    "    clf = SVC(kernel='rbf', gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    kf_info[split] = {\"Train Indices\": train_index,\n",
    "                      \"Val Indices\": val_index,\n",
    "                      \"Train Report\": classification_report(y_true=y_train, y_pred=clf.predict(X_train)),\n",
    "                      \"Val Report\": classification_report(y_true=y_val, y_pred=clf.predict(X_val)),\n",
    "                      \"Test Report\": classification_report(y_true=y_test, y_pred=clf.predict(X_test))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(kf_info)):\n",
    "    print(\"Fold Number = \", k)\n",
    "    print(\"Train Report\\n\", kf_info[0][\"Train Report\"])\n",
    "    print(\"Validation Report\\n\", kf_info[0][\"Val Report\"])\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the performance doesn't really change for each of the splits. This is great news! It demonstrates that our SVM might actually be performing well as a classifier. Of course, we expect that it would. Even K-Means seems to group the data pretty strongly. So, what if we wanted to make a model and export it?\n",
    "\n",
    "Let's say we think fold 3 in our 10-fold cross validation gave us the best results, and we wanted to build a model based on that. Thankfully, we kept a log of the indices from each fold, so we can go back and fetch them again. All we need to do is re-train a classifier using those indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = kf_info[3][\"Train Indices\"]\n",
    "\n",
    "X_train, y_train = X[train_index], y[train_index]\n",
    "\n",
    "clf = SVC(kernel='rbf', gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_hat = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_hat)\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5), dpi=150)\n",
    "vis.pretty_cm(cm, ax0=ax, labelnames=['Cr', 'In', 'PS', 'Pa', 'RS', 'Sc'])\n",
    "ax.set_title('SVM Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classifier using Pickle\n",
    "CLF_PATH = os.path.join('..', 'models', 'SVM_classifier.pkl')\n",
    "\n",
    "with open(CLF_PATH, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Load the classifier later as\n",
    "\n",
    "# import pickle\n",
    "# CLF_PATH = os.path.join('..', 'models', 'SVM_classifier.pkl')\n",
    "# with open(CLF_PATH, 'rb') as f:\n",
    "#     clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We've covered a couple of simple concepts in supervised learning, but I must warn you that there are more pitfalls that you might encounter. You might find yourself dealing with data where your input variables have wildly different ranges and variances. To counter this, you may be interested in using something like `sklearn.preprocessing.StandardScaler()` to standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "In this notebook we only covered one type of supervised learning tool: the support vector machine (SVM). Specifically, we were using a support vector classifier (SVC). SVMs are nice because they are quick to train and they look for linear separations in data. If you recall from our PCA lecture, we learned that the *kernel trick* can be applied to SVMs. And if you look closely at the code we covered in this notebook, you might recognize that we applied the *kernel trick* here. \n",
    "\n",
    "## Further Exploration\n",
    "Now that you've had some time to learn about some of these methods, it's time for a couple of more challenging tasks. Some of these questions are more open-ended than before, so it's up to you to be creative in your interpretation and approach. I've ordered these by what I believe is increasing difficulty.\n",
    "\n",
    "- What happens if we don't apply the *kernel trick*?\n",
    "    - How do we change the code above to only look for linear separations?\n",
    "- What happens if we train on other features we have?\n",
    "    - You've already explored unsupervised learning on the raw features, histogram features, etc. Now try applying this method to the various features. What do you expect will happen?\n",
    "- How do you think performance will change by varying the train/val/test splits?\n",
    "    - Try various splits and show your results.\n",
    "- The final block of code that prints out classification performance metrics for each split doesn't visualize the method very well.\n",
    "    - Make your own visualization tool to make a plot (or several plots) that show classification performance.\n",
    "- There are a handful of misclassifications.\n",
    "    - Which ones are misclassified?\n",
    "    - How are they misclassified?\n",
    "    - Visualize it.\n",
    "- There's an extension to K-Fold Cross Validation called *Leave-One-Out Cross Validation*.\n",
    "    - Look it up and implement it into your own project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
