{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0449fde-2407-435e-8cd0-eb87062550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goal here is to implement a bunch of shareable image processing functions and see how\n",
    "the differently processed images affect the accuracy of the machine learning. I'll use\n",
    "one relatively standard machine learning algorithm with multiple different pre-processing\n",
    "methods to evaluate their effect on the accuracy.\n",
    "\n",
    "File is split into two parts: Generic image functions, and script to run analysis\n",
    "\"\"\"\n",
    "\n",
    "############################\n",
    "#Imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "############################\n",
    "#Generic image processing functions\n",
    "\n",
    "#Converts any-channel image (represented as numpy array) to n-channel image by slicing off extras\n",
    "def truncateChannels(img, n=3):\n",
    "    rows, cols, _ = img.shape\n",
    "    result = np.empty((rows, cols, n))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            result[row, col] = img[row, col][:n]    \n",
    "    return result\n",
    "            \n",
    "#Crops image (represented as numpy array) for input to VGG16 (and others) in most basic way possible;\n",
    "#Starts at top-left corner and crops original image down to one by just slicing off excess\n",
    "def singleCrop(img, size=224):\n",
    "    return img[:size, :size]\n",
    "\n",
    "#Crops image (represented as numpy array) into as many smaller images as possible for input to VGG16\n",
    "#while maintaining no overlap between them. So it divides the image into 224 x 224 sections\n",
    "#Returns these smaller images together in a one-dimensional list\n",
    "def multiCrop(img, size=224):\n",
    "    rows, cols, _ = img.shape\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    #Cut off the last increments since they will be less than [size] from the edge of the image\n",
    "    #Note the +1s handle an off-by-one error in the edge case where image dimensions are an\n",
    "    #integer multiple of [size]\n",
    "    for i in (range(0, rows + 1, size)[:-1]):\n",
    "        for j in (range(0, cols + 1, size)[:-1]):\n",
    "            result.append(img[i:i + size, j:j + size])\n",
    "            \n",
    "    return result\n",
    "\n",
    "#Pretty self-explanatory; assumes the input is a list\n",
    "def flatten(L):\n",
    "    result = []\n",
    "    for subL in L:\n",
    "        for item in subL:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "#Rounds the integer n to the nearest multiple of increment\n",
    "def roundTo(n, increment):\n",
    "    excess = n % increment\n",
    "    if excess == 0:\n",
    "        return n\n",
    "    else:\n",
    "        if (increment - excess) > excess:\n",
    "            return n - excess\n",
    "        else:\n",
    "            return n - excess + increment\n",
    "        \n",
    "#Keeps track of how many times a function is called, useful for a variety of things\n",
    "#Add keyword argument callNum to the function to be wrapped\n",
    "class FnWrapper(object):\n",
    "    def __init__(self, fn):\n",
    "        self.calls = -1 #i.e. first call will be the 0th\n",
    "        self.fn = fn\n",
    "        \n",
    "    def call(self, *args, **kw):\n",
    "        self.calls += 1\n",
    "        return self.fn(*args, **kw, callNum=self.calls)\n",
    "        \n",
    "def wrapFn(fn):\n",
    "    wrapper = FnWrapper(fn)\n",
    "    return lambda *args, **kw : wrapper.call(*args, **kw)\n",
    "\n",
    "#Apply multiCrop, then truncate alpha channel on all of the resulting images\n",
    "def truncateMulti(original):\n",
    "    return list(map(lambda img : truncateChannels(img), multiCrop(original)))\n",
    "\n",
    "#### All of the following functions will be designed so that they just take in the image path,\n",
    "#so they can all be used in the same spot in my script and I can interchange them easily.\n",
    "#They each comprise a different procedure for getting from path to all the final images that\n",
    "#can be extracted from that path.\n",
    "\n",
    "#Simply crop the original image into as many smaller images as possible without changing\n",
    "#any of its information\n",
    "def standardPrep(path):\n",
    "    original = np.array(Image.open(path))\n",
    "    return list(map(lambda img : truncateChannels(img), multiCrop(original)))\n",
    "\n",
    "#Resize the original image to the nearest multiple of 224 (or whatever size) so no information\n",
    "#is left \"unused\".\n",
    "#The default filter used for resizing is a \"bicubic\" filter, but this can be changed. May be\n",
    "#worth trying different types if this ends up distorting results of training significantly\n",
    "def resizePrep(path, size=224):\n",
    "    original = Image.open(path)\n",
    "    w, h = original.width, original.height\n",
    "    newW, newH = roundTo(w, size), roundTo(h, size)\n",
    "    resized = np.array(original.resize((newW, newH)))\n",
    "    return list(map(lambda img : truncateChannels(img), multiCrop(resized)))\n",
    "\n",
    "#Uniformly rotate some of the images (by increments of 90 degrees) resulting from regular\n",
    "#multicropping, so the data is changed but none of it is duplicated\n",
    "def uniformRotatePrep(path):\n",
    "    original = np.array(Image.open(path))\n",
    "    images = list(map(lambda img : truncateChannels(img), multiCrop(original)))\n",
    "    for i in range(len(images)):\n",
    "        angle = (i % 4) * 90\n",
    "        #Just to help speed things up\n",
    "        if angle == 0:\n",
    "            continue\n",
    "        old = images[i]\n",
    "        asImage = Image.fromarray(np.uint8(old))\n",
    "        new = np.array(asImage.rotate(angle))\n",
    "        images[i] = new\n",
    "    return images\n",
    "\n",
    "#Rotate all of the original images 3 times and then use regular multicropping, so there may be\n",
    "#some slightly different data if the dimensions aren't an even multiple of 224 (or whatever size)\n",
    "#but overall the uniqueness of the images is dubious\n",
    "def multiRotatePrep(path):\n",
    "    original = Image.open(path)\n",
    "    images = truncateMulti(np.array(original))\n",
    "    for i in range(1, 4):\n",
    "        rotated = original.rotate(90 * i)\n",
    "        images.extend(truncateMulti(np.array(rotated)))\n",
    "    \n",
    "    return images\n",
    "\n",
    "#Apply a bilateral filter over the original images and then crop them using regular multicropping\n",
    "def bilateralPrep(path, diameter=9, sigmaColor=100, sigmaSpace=100):\n",
    "    original = Image.open(path)\n",
    "    truncated = np.uint8(truncateChannels(np.array(original)))\n",
    "    #I played a bit with these parameters and looking at the results before settling on these\n",
    "    #values, could be worth investigating them further\n",
    "    filtered = cv2.bilateralFilter(truncated, diameter, sigmaColor, sigmaSpace)\n",
    "    return multiCrop(filtered)\n",
    "\n",
    "#Apply a dilation filter over the original images and then crop them using regular multicropping\n",
    "def dilatePrep(path, kernelShape=cv2.MORPH_CROSS, kernelSize=(3, 3)):\n",
    "    original = Image.open(path)\n",
    "    truncated = np.uint8(truncateChannels(np.array(original)))\n",
    "    #kernelShape can be one of cv2.MORPH_RECT, cv2.MORPH_CROSS, and cv2.MORPH_ELLIPSE\n",
    "    #Chose one sort of arbitrarily, seemed like the least distorted image of the 3 options, would\n",
    "    #be worth looking into their effects more in the future\n",
    "    #kernelSize chosen equally arbitrarily, also can choose to make it not square...\n",
    "    kernel = cv2.getStructuringElement(kernelShape, kernelSize)\n",
    "    filtered = cv2.dilate(truncated, kernel)\n",
    "    return multiCrop(filtered)\n",
    "\n",
    "#Apply histogram equalization over the original images and then crop them using regular multicropping\n",
    "#Strongly suspect it is not going to be helpful at all for this particular dataset, but we'll try it\n",
    "def equalizePrep(path):\n",
    "    original = Image.open(path).convert(mode=\"L\")\n",
    "    converted = np.uint8(np.array(original))\n",
    "    equalized = cv2.equalizeHist(converted)\n",
    "    #This is super scuffed but... meh\n",
    "    reconverted = np.array(Image.fromarray(equalized).convert(mode=\"RGB\"))\n",
    "    return multiCrop(reconverted)\n",
    "\n",
    "\"\"\"\n",
    "Alright, so here's the one that got me the best results. I'll try to include as thorough of a\n",
    "description of what it actually does to the image as possible, as well as outline all of the\n",
    "parameters that can be changed and likely have some more room for optimization.\n",
    "\n",
    "Steps:\n",
    "1. Truncate alpha channel, takes image from 10XX x 10XX x 4 to 10XX x 10XX x 3. Saw after I\n",
    "had already written this that there are built-in conversions in PIL and stuff, I don't do anything\n",
    "special so it shouldn't be any different from those.\n",
    "2. Apply a bilateral filter. You may have used this before, but I'll explain what it is just in case:\n",
    "Essentially the idea of it is to reduce miscellaneous noise in an image. At every pixel it gets the\n",
    "weighted average of the intensity values of pixels around it in a box of [diameter] x [diameter] and\n",
    "then replaces the original intensity with that weighted average. Can just check wikipedia for more on\n",
    "that: https://en.wikipedia.org/wiki/Bilateral_filter\n",
    "3. Rotate images uniformly. I used a really funky wrapper function to track how many times I called\n",
    "this if you look into how I used it below. Main idea is just that I make sure to rotate the same\n",
    "proportion of the images by each angle, only used increments of 90 degrees of course.\n",
    "4. Resize image using PIL's resize method, which tries to preserve as much of the data as possible. I\n",
    "didn't try changing the resampling filter (using resample=<int> as a keyword argument) but toyed with\n",
    "the idea of doing that if I didn't get decent results, may be worth trying?\n",
    "Details on changing that are here:\n",
    "https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize\n",
    "https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-filters\n",
    "\n",
    "Parameters:\n",
    "\n",
    "path -- only actually required parameter, should be a path to the image you want to process from\n",
    "current directory, I just feed it to PIL.Image.open so if your path works for that it works here\n",
    "\n",
    "diameter -- size of the box (in pixels) used for averaging pixel intensities in the bilateral filter.\n",
    "In general a larger diameter should always yield a better result, but make it take longer.\n",
    "\n",
    "sigmaColor and sigmaSpace -- I'm not sure what exactly the mathematical interpretation of these two\n",
    "is, but I can tell you what the openCV docs said about them. Generally they should both be the same\n",
    "value, and should be in the range [10, 150]. Lower than 10 and your filter will have little to no\n",
    "effect, higher than 150 and all of your edges will become HUGE.\n",
    "\n",
    "finalSize -- size of the output image in pixels, only included this in case it needs to be a different\n",
    "size for input into different CVNs like Alexnet, etc.\n",
    "\n",
    "callNum -- used to determine how many times the function has been called in order to make sure we're\n",
    "rotating the same number of images each direction, if you set it to 0 every time it won't rotate them\n",
    "\"\"\"\n",
    "\n",
    "def optimalPrep(path, diameter=9, sigmaColor=100, sigmaSpace=100, finalSize=224, callNum=0):\n",
    "    #Step 1, nothing actually interesting happening here\n",
    "    original = Image.open(path)\n",
    "    truncated = np.uint8(truncateChannels(np.array(original)))\n",
    "    \n",
    "    #Step 2, apply bilateral filter\n",
    "    filtered = cv2.bilateralFilter(truncated, diameter, sigmaColor, sigmaSpace)\n",
    "    \n",
    "    #Step 3, rotate\n",
    "    angle = (callNum % 4) * 90\n",
    "    asImage = Image.fromarray(filtered)\n",
    "    \n",
    "    #Step 4, resize\n",
    "    resized = asImage.resize((finalSize, finalSize))\n",
    "    \n",
    "    #Others all return a list, this is just specific to how I had these set up in\n",
    "    #my loop. So I have to return a singleton list here\n",
    "    return [np.array(asImage.rotate(angle))]\n",
    "\n",
    "############################\n",
    "#Script to run the actual analysis on THIS data\n",
    "\n",
    "#Load the excel file\n",
    "csvPath = os.path.join(\"..\", \"data\", \"TMST\", \"TMST_label.csv\")\n",
    "with open(csvPath, 'r') as f:\n",
    "    contents = f.read()\n",
    "    labelDict = dict()\n",
    "    for line in contents.splitlines()[1:]:\n",
    "        imgID, label = line.split(',')\n",
    "        labelDict[imgID] = label\n",
    "\n",
    "#Let's make a helper function to help us find data files\n",
    "def get_files(file_directory, extension='*.bmp'):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        file_directory: path to directory to search for files\n",
    "        extension: desired file type, default *.bmp (bitmap)\n",
    "    \n",
    "    Return:\n",
    "        files: list of files in file_directory with extension\n",
    "    \"\"\"\n",
    "    files = glob.glob(os.path.join(file_directory, extension))\n",
    "    return files\n",
    "\n",
    "#Remove all characters but the ID digits\n",
    "def onlyDigits(s):\n",
    "    result = \"\"\n",
    "    filtered = [c for c in s if c.isdigit()]\n",
    "    for c in filtered:\n",
    "        result += c\n",
    "    return result\n",
    "\n",
    "#Extract the image ID from its path\n",
    "def getID(prefix, path):\n",
    "    return onlyDigits(path[:-4].split(prefix)[1])\n",
    "\n",
    "# define our model\n",
    "class VGG_fc1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_fc1, self).__init__()\n",
    "        self.features = vgg16(pretrained=True).features # convolutional layers\n",
    "        self.avgpool = vgg16(pretrained=True).avgpool\n",
    "        self.fc1 = vgg16(pretrained=True).classifier[0] # first layer of classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Extract first fully connected feature vector\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = VGG_fc1().eval() # turn model into evaluation mode\n",
    "\n",
    "# transform from image to the input of CNN, follow the same procedure as ImageNet\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert pixel values to the range of [0,1]\n",
    "    # normalize the pixel values according to the mean and std of ImageNet\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "#I have modified this so that it now expects as input a numpy array of size (224 x 224 x 3), which will be\n",
    "#the result type of many of my image pre-processing functions\n",
    "def get_feature(img):\n",
    "    '''Run a pytorch Tensor through VGG16 and get feature vector '''\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0) # make the tensor into single batch tensor with shape [1, 3, 224, 224]\n",
    "    img = img.to(dtype=torch.float)\n",
    "    feature = model(img) # get feature\n",
    "    feature = feature.detach().numpy() # detach the gradient, convert to numpy array\n",
    "    return feature.flatten()\n",
    "\n",
    "dataPath = os.path.join(\"..\", \"data\", \"TMST\", \"images\")\n",
    "imagePaths = get_files(dataPath, extension=\"*.png\")\n",
    "\n",
    "data = pd.DataFrame(data=imagePaths, columns=[\"imagePath\"])\n",
    "data[\"imageID\"] = [i for i in map(lambda s : getID(dataPath, s), data[\"imagePath\"])]\n",
    "data[\"label\"] = [labelDict[i] for i in data[\"imageID\"]]\n",
    "\n",
    "def processSet(prepFn, exclusions=[]):\n",
    "    expandedIDs = []\n",
    "    expandedLabels = []\n",
    "    expandedVectors = []\n",
    "    #labelCounts = dict()\n",
    "    for i in tqdm(data.index):\n",
    "        #If I crop every single image into as many as possible we get... way too many to do on my laptop.\n",
    "        #So I'll reduce all of them to the same amount as the classes with the least images (5)\n",
    "        curLabel = data.loc[i][\"label\"]\n",
    "\n",
    "        \"\"\"\n",
    "        if curLabel in labelCounts:\n",
    "            if labelCounts[curLabel] >= 5:\n",
    "                continue\n",
    "            else:\n",
    "                labelCounts[curLabel] += 1\n",
    "        else:\n",
    "            labelCounts[curLabel] = 1\n",
    "        \"\"\"\n",
    "        if curLabel in exclusions:\n",
    "            continue\n",
    "\n",
    "        preppedImages = prepFn(data.loc[i][\"imagePath\"])\n",
    "        vectors = [get_feature(img) for img in preppedImages]\n",
    "        expandedVectors.append(vectors)\n",
    "\n",
    "        numAdded = len(preppedImages)\n",
    "        expandedIDs.append([data.loc[i][\"imageID\"]] * numAdded)\n",
    "        expandedLabels.append([curLabel] * numAdded)\n",
    "\n",
    "    expandedIDs = flatten(expandedIDs)\n",
    "    expandedLabels = flatten(expandedLabels)\n",
    "    expandedVectors = flatten(expandedVectors)\n",
    "    assert(len(expandedIDs) == len(expandedLabels) == len(expandedVectors))\n",
    "\n",
    "    expandedData = pd.DataFrame(data=list(zip(expandedIDs, expandedLabels, expandedVectors)),\n",
    "                               columns=[\"imageID\", \"label\", \"featureVector\"])\n",
    "    print(prepFn.__name__, len(expandedData[\"featureVector\"]))\n",
    "    savePath = os.path.join(\"..\", \"data\", \"TMST\", prepFn.__name__ + \".pkl\")\n",
    "    expandedData.to_pickle(savePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039735c8-1538-4517-a288-07527597db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepFns = [wrapFn(optimalPrep)]\n",
    "#wrapFn is just to count function calls\n",
    "lackData = \"A,B,C,D,E,F,G,H,I,J,K,L\".split(',')\n",
    "#Excluded all of the labels with only 5 images\n",
    "for fn in prepFns:\n",
    "    processSet(fn, exclusions=lackData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f217d32-2b25-4a96-bba0-15e4cdcf387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from helper import visualize as vis\n",
    "\n",
    "def doAnalysis(picklePath):\n",
    "    labelSubset = \"M,N,O,P,Q,R,S,T,U,V,W\".split(',')\n",
    "    df = pd.read_pickle(picklePath)\n",
    "    print(len(df[\"featureVector\"]))\n",
    "    \n",
    "    #Extra part for separating out only the classes with enough data\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for i in range(len(df[\"label\"])):\n",
    "        if df[\"label\"][i] in labelSubset:\n",
    "            vectors.append(df[\"featureVector\"][i])\n",
    "            labels.append(df[\"label\"][i])\n",
    "    print(len(vectors))\n",
    "    trainFeatures, testFeatures, trainLabels, testLabels = train_test_split(vectors,\n",
    "                                                                           labels,\n",
    "                                                                           test_size=0.2,\n",
    "                                                                           random_state=617,\n",
    "                                                                           shuffle=True)\n",
    "    trainFeatures = np.array(list(trainFeatures))\n",
    "    testFeatures = np.array(list(testFeatures))\n",
    "    \n",
    "    pca50 = PCA(n_components=50, svd_solver=\"randomized\", random_state=616)\n",
    "    pca50.fit(trainFeatures)\n",
    "    trainFeatures = pca50.transform(trainFeatures)\n",
    "    testFeatures = pca50.transform(testFeatures)\n",
    "    \n",
    "    classifier = SVC(kernel=\"rbf\", gamma=\"auto\", random_state=618)\n",
    "    classifier.fit(trainFeatures, trainLabels)\n",
    "    \n",
    "    root = picklePath[:-4]\n",
    "    savePath = root + \"cm\"\n",
    "    print(root + \" score: \" + str(classifier.score(testFeatures, testLabels)))\n",
    "    cm = confusion_matrix(y_true=testLabels, y_pred=classifier.predict(testFeatures))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(24, 24), dpi=150)\n",
    "    vis.pretty_cm(cm, ax0=ax, labelnames=labelSubset)\n",
    "    ax.set_title(root + \" Confusion Matrix\")\n",
    "    plt.savefig(savePath)\n",
    "    #train, get score, maybe a confusion matrix...\n",
    "    \n",
    "loadPath = os.path.join(\"..\", \"data\", \"TMST\")\n",
    "paths = [os.path.join(loadPath, \"wrapFn\".pkl\")]\n",
    "for item in paths:\n",
    "    doAnalysis(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
